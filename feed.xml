<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://ccolombe12.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ccolombe12.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-10-16T21:38:41+00:00</updated><id>https://ccolombe12.github.io/feed.xml</id><title type="html">blank</title><subtitle>The academic wepage of Connor Colombe
</subtitle><entry><title type="html">Alice and Bob Play a Guessing Game</title><link href="https://ccolombe12.github.io/blog/2023/Alice_and_Bob_Guessing_game/" rel="alternate" type="text/html" title="Alice and Bob Play a Guessing Game" /><published>2023-10-16T00:00:01+00:00</published><updated>2023-10-16T00:00:01+00:00</updated><id>https://ccolombe12.github.io/blog/2023/Alice_and_Bob_Guessing_game</id><content type="html" xml:base="https://ccolombe12.github.io/blog/2023/Alice_and_Bob_Guessing_game/"><![CDATA[<style type="text/css">
    ol { list-style-type: lower-alpha; }
</style>

<p>Here is another cool problem I stumbled upon the other day.</p>
<h2 id="problem-statement">Problem Statement</h2>
<p>Bob chooses a integer uniformly at random between 1 and 1000. Alice has to guess the chosen number as quickly as possible. Bob will let Alice know whether her guess is smaller than, larger than, or equal to his number. If Alice’s guess is smaller than Bob’s number, Bob replaces the number with another integer chosen uniformly at random from $[1,1000]$. Prove that there exists a strategy that Alice can use to finish the game in such a way that the expected number of steps is smaller than 45.</p>
<hr />

<h2 id="solution">Solution</h2>

<p>Let’s generalize and suppose Bob picks integers uniformly from the set \([N] = \{1,2,3,\ldots, N \}\). I initially set out to try to determine the <em>optimal</em> strategy using dynamic programming. Let \(E_k\) be the expected number of turns to finish the game given that we know that Bob’s current number is at most \(k\).</p>

<p>We can set up the recurrence</p>

\[\begin{equation}
E_k = 1 + \min_{j \in [k]}\left(\underbrace{\frac{k-j}{k}E_N}_{\text{guess too low}} + \underbrace{\frac{1}{k} \cdot 0}_{\text{guess correct}} + \underbrace{\frac{j-1}{k}E_j}_{\text{guess too high}}\right) \quad \forall k \in [N].
\end{equation}\]

<p>But since every \(k\) relies on \(E_n\), the quantity we are trying to solve for, this ends up to not be as straightforwards to evaluate as we might hope.</p>

<p>What else can we try? Well this problem is sort of similar to the <a href="https://brilliant.org/wiki/egg-dropping/#n-eggs-k-floors">egg drop problem</a> with one egg and each time it breaks we start with a new building with a randomly chosen number of floors. In the egg drop problem with two eggs, if the first egg breaks on the first floor tested, \(x\) then the next drops must necessarily be \(x-1, x-2, \ldots, 1\) until the least-breaking floor is found. Let’s try to build a heuristic strategy off this intuition.</p>

<p>We can pick some threshold \(\ell\) and use the strategy:</p>
<ul>
  <li>Guess \(\ell\) repeatedly until either we guess correct or Bob rolls a number less than \(\ell\).</li>
  <li>Guess down from \(\ell, \ell - 1, \ldots, 1\) until we hit the target</li>
</ul>

<p>The number of turns until the first step succeeds is a geometric random variable with parameter \(\ell/N\) and thus the expected number of turns until we succeed is \(N / \ell\). Now, given that the target number is at most \(\ell\), it has a uniform distribution over \(1,\ldots,\ell\) and thus the expected number of guesses needed to reach it is:
\(\begin{align*}
  \mathbb{E}[\text{# steps needed given step 1 succeeds}]&amp; = \underbrace{\frac{1}{\ell}\cdot 0}_{x = \ell} + \underbrace{\frac{\ell-1}{\ell}\left(\frac{1}{\ell - 1}\sum_{k=1}^{\ell-1} k\right)}_{x &lt; \ell}\\ 
  &amp; = \frac{\ell-1}{2}.
\end{align*}\)</p>

<p>Putting it together, let \(f(\ell)\) be the expected number of turns to guess Bob’s number under our strategy with threshold number \(\ell\). This is given by</p>

\[\begin{equation}
f(\ell) = \frac{N}{\ell} + \frac{\ell-1}{2}. \label{eq: f}
\end{equation}\]

<p>We would like to minimize this function. Taking the derivative, we find a single critical point at \(\boxed{\ell^* = \sqrt{2 N}}\), and the second derivative is positive for all $\ell &gt; 0$ which implies we have found a global minimum for our problem. Plugging this value into (\ref{eq: f}), we have that</p>

<p>\(\begin{equation}
  \boxed{f(\ell^*) = \sqrt{2 N} - \frac{1}{2}}
\end{equation}\)
which for \(N = 1000\) gives us an expected number of moves of \(\approx\) 44.2214 which is less than 45, as desired. This result is quite interesting! For large $N$, on average will will only need to guess \(\mathcal{O}\left(\frac{1}{\sqrt{N}}\right)\) fraction of the numbers.</p>]]></content><author><name>Connor Colombe</name></author><category term="math" /><summary type="html"><![CDATA[Here is another cool problem I stumbled upon the other day. Problem Statement Bob chooses a integer uniformly at random between 1 and 1000. Alice has to guess the chosen number as quickly as possible. Bob will let Alice know whether her guess is smaller than, larger than, or equal to his number. If Alice’s guess is smaller than Bob’s number, Bob replaces the number with another integer chosen uniformly at random from $[1,1000]$. Prove that there exists a strategy that Alice can use to finish the game in such a way that the expected number of steps is smaller than 45.]]></summary></entry><entry><title type="html">A Dice Game</title><link href="https://ccolombe12.github.io/blog/2023/Dice_game/" rel="alternate" type="text/html" title="A Dice Game" /><published>2023-10-14T00:00:00+00:00</published><updated>2023-10-14T00:00:00+00:00</updated><id>https://ccolombe12.github.io/blog/2023/Dice_game</id><content type="html" xml:base="https://ccolombe12.github.io/blog/2023/Dice_game/"><![CDATA[<style type="text/css">
    ol { list-style-type: lower-alpha; }
</style>

<p>It’s no secret that I really enjoy solving interview brainteaser problems. I recently stumbled across a problem from a Jane Street Interview that I thought was interesting.</p>
<h2 id="problem-statement">Problem Statement</h2>
<p>The original problem is as follows: You are playing a game with a fair \(n\) sided dice, with faces 1 through \(n\) and \(T\) turns. The die starts with side 1 facing up. On each turn, you may either</p>

<ol>
  <li>Receive a payment equal to the value of the current face up side of the dice</li>
  <li>Re-roll the dice.</li>
</ol>

<p>The question is: What is the optimal strategy, and what is the most you would be willing to pay to play this game?</p>

<p>In this blog post we will consider a continuous version of the game and discuss how our insights would then carry over to the discrete case.</p>

<p>In our continuous case, instead of a discrete dice, we will play with a “dice” that acts as a uniform random number generator from \([0,1]\). We will also assume that we begin with a face up value of zero.</p>
<hr />

<h2 id="solution">Solution</h2>

<p>Before diving into calculations, we can think a about what form the optimal strategy may take. Our first insight is that on any given turn, if the current “face-up” side of the dice is very close to 1, we should probably choose to receive payment. Furthermore, if we are on our last turn, it doesn’t matter what side of the dice is up, it is optimal to choose to receive the payment. We can begin to reason that for \(k\) turns left, the optimal strategy might have some threshold function \(v_k\) where if on the \(k\)-th to last turn the face up value is \(x\) then it is optimal to receive payment only if \(x \geq v_k\) and it is optimal to re-roll otherwise.</p>

<p>First let’s try to show at there exists this threshold function. Let \(w(v,k)\) be the expected value of the game, under optimal play, starting with value \(v\) face up and having \(k\) turns left.</p>

<p>We have that \(w(v,0)= 0\) and a recurrence relation</p>

\[\begin{equation}
w(v,k)= \max\left\{\underbrace{v + w(v, k - 1)}_{\text{receive payout}}, \; \underbrace{\int_0^1 w(x, k - 1) \; dx}_{\text{re-roll}}\right\}.\label{eq: cont rec.}
\end{equation}\]

<p>We claim that \(w(v,k)\) is a continuous function in \(v\). This can be proven by induction on \(k\) and noting that taking the max of continuous functions preserves continuity.</p>

<p>Now, we claim that \(w(v,k)\) is a non-decreasing function in \(v\). This can again be proven by induction on \(k\) using (\ref{eq: cont rec.}). With these, If we consider the cases where \(v = 0\) and \(v = 1\), we see that</p>

<p>\(w(0,k-1) \leq \int_0^1 w(x, k - 1) \; dx\) and \(1 +w(1,k-1) &gt; \int_0^1 w(x, k - 1)\). It then follows by the fact that \(w(v,k)\) is continuous and non-decreasing and the intermediate value theorem that for each \(k\) there is some smallest value of \(v \in [0,1]\) such that receiving payout is optimal if the face up side is at least this value. It remains to find the for each \(k\) what this threshold \(v_k\) is.</p>

<p>We can reason that \(v_k\) ought to be an increasing in \(k\). That is, if you are willing to receive a payout of \(x\) with \(k\) turns remaining, then you would also be willing to accept it with at most \(k-1\) turns remaining. With this in mind, we can begin to derive a recurrence relation for \(v_k\).</p>

<p>Equation (\ref{eq: cont rec.}) and the existence of \(v_k\) imply that we have the equality \(\begin{equation}
v_k + w(v_k, k-1) = \int_0^1 w(x, k - 1) \;  \label{eq:v_k equal}.
\end{equation}\)</p>

<p>However, since \(v_k\) is an non-decreasing function, we have that \(v_k + w(v_k, k-1) =  k v_k\) (meaning that if we we take payout, we will keep taking that payout every turn until the game ends). If we denote \(I_k := \int_0^1 w(x, k)\), this give us the relation</p>

\[\begin{equation}
\boxed{k v_k = I_{k-1}} \label{eq:I_k def}
\end{equation}\]

<p>We can then expand the RHS of (\ref{eq:v_k equal}) as</p>

\[\begin{align*}
\int_0^1 w(x, k - 1) \; dx &amp; = \int_0^{v_{k-1} }w(x, k - 1)\; dx + \int_{v_{k-1} }^1 w(x, k - 1)\; dx\\ 
                    &amp; = \int_0^{v_{k-1} }\left(I_{k-2}\right)\; dx+ \int_{v_{k-1} }^1 (k-1) x \; dx\\ 
                    &amp; = v_{k-1} I_{k-2} + \frac{k-1}{2}\left(1 - v_{k-1} ^2\right)\\ 
                    &amp; = (k-1)v_{k-1}^2 + \frac{k-1}{2}\left(1 - v_{k-1} ^2\right)\\ 
                    &amp; = \frac{k-1}{2}\left(1 + v_{k-1}^2\right).
\end{align*}\]

<p>If we then combine this with (\ref{eq:v_k equal}) and (\ref{eq:I_k def}), we have a recurrence relation for \(v_k\)</p>

\[\begin{equation}
\boxed{v_k = \frac{k-1}{2k}\left(1 + v_{k-1}^2\right), \quad v_1 = 0} \label{eq:v_k req rel}.
\end{equation}\]

<p>Unfortunately, we cannot solve for a closed form solution to (\ref{eq:v_k req rel}). But we can solve for the numerical value of our game for a given number of turns. Starting with a current value of zero and \(T\) turns, the value of the game is given by \(w(0,T)\). Since $v_k &gt; 0$ for all \(k &gt; 1\), the optimal first move is to re-roll and which give us a value of 
\(\begin{equation}
  w(0,T) = \int_0^1 w(x, T-1) = I_{T-1} = T v_T
\end{equation}\)</p>

<p>Which means that the value of our game with \(T\) turns is \(\boxed{w(0,T)= Tv_T}\).</p>

<p>We can say a few more things about the continuous case before concluding with a few remarks on what the optimal strategy would be in the discrete case. Recall that \(v_k\) is increasing in \(k\). It is also bound above by 1. Therefore the monotone convergence theorem implies that \(v_k\) converges. In the limit we find that \(v_k\to 1\) which matches our intuition that with many turns remaining, we are willing to wait for a value very close to 1. Below are plots of \(v_k\) for \(k\leq 50\) and the value of the game for \(T \leq 50\)</p>

<h3><figure><center>
  <img width="300" src="/assets/img/blog_images/Dice_game/test.svg" class="img-fluid rounded z-depth-1" zoomable="true" />
</center></figure></h3>
<div class="caption">
    Fig 1: Plotting $v_k$ for for all $k \leq 50$.
</div>

<h3><figure><center>
  <img width="300" src="/assets/img/blog_images/Dice_game/game_vals.svg" class="img-fluid rounded z-depth-1" zoomable="true" />
</center></figure></h3>
<div class="caption">
    Fig 2: Plotting $w(0,T)$ for for all $T \leq 50$.
</div>

<p>To conclude, how might we use what we have found to tackle the discrete case? The analysis is exactly the same but now the cutoff thresholds will be discrete. The recurrence relation in (\ref{eq: cont rec.}) will now be</p>

<p>\(\begin{equation}
w(v,k)= \max\left\{\underbrace{v + w(v, k - 1)}_{\text{receive payout}}, \; \underbrace{\frac{1}{n} \sum_{u=1}^n w(u, k - 1)}_{\text{re-roll}}\right\}.\label{eq: disc rec.}
\end{equation}\)
Otherwise, the insights from the continuous case still hold!</p>]]></content><author><name>Connor Colombe</name></author><category term="math" /><summary type="html"><![CDATA[It’s no secret that I really enjoy solving interview brainteaser problems. I recently stumbled across a problem from a Jane Street Interview that I thought was interesting. Problem Statement The original problem is as follows: You are playing a game with a fair \(n\) sided dice, with faces 1 through \(n\) and \(T\) turns. The die starts with side 1 facing up. On each turn, you may either]]></summary></entry><entry><title type="html">Generalizing the “Drunk Pasenger” Problem</title><link href="https://ccolombe12.github.io/blog/2023/Drunk_passenger/" rel="alternate" type="text/html" title="Generalizing the “Drunk Pasenger” Problem" /><published>2023-08-16T00:00:01+00:00</published><updated>2023-08-16T00:00:01+00:00</updated><id>https://ccolombe12.github.io/blog/2023/Drunk_passenger</id><content type="html" xml:base="https://ccolombe12.github.io/blog/2023/Drunk_passenger/"><![CDATA[<style type="text/css">
    ol { list-style-type: lower-alpha; }
</style>

<p>I recently stumbled across a problem that asked for a generalization of the “Drunk Passenger” problem. The classic problem is as follows:</p>
<h2 id="problem-statement">Problem Statement</h2>
<p>There are \(n\) seats on an airplane and \(n\) passengers. The passengers are in a line to board the plane and the \(i\)-th passenger in line is assigned to seat number \(i\) on the plane. However, the first passenger in line is drunk and takes a seat on the plane uniformly at random. The rest of the passengers take a on the plane seat one by one. Passenger \(i \geq 2\) will sit in their assigned seat if it is available, otherwise they will sit in an open seat uniformly at random. What is the probability the \(n\)-th passenger in line sits in their assigned seat?</p>

<p>The version of the problem we will solve is: find the probability that the \(k\)-th passenger in line sits in their assigned seat for \(k = 1,\ldots,n\).</p>
<hr />

<h2 id="solution">Solution</h2>

<p>Let \(P(n, k)\) be the probability that the \(k\)-th passenger sits in their assigned seat. Let’s do a little exploration. We can immediately reason that because the first passenger (\(k=1\)) always picks their seat uniformly at random, we have that \(P(n,1) = 1/n\) for all \(n\). But what about for \(k=2\) ? The second passenger will sit in their assigned seat only if the first passenger does not steal their seat. This implies \(P(n,2) = \frac{n-1}{n}\). For \(k \geq 3\), we rely on an important observation:</p>

<blockquote>
  <p>If the first passenger sits in seat \(i\) such that \(3 \leq  i \leq n\), then the passengers \(2,\ldots, i - 1\) will all sit in their assigned seats.</p>
</blockquote>

<p>We can now find \(P(n,k)\) by conditioning on three events:</p>

<ol>
  <li>The first passenger sits in seat \(1\).</li>
  <li>The first passenger sits in seat \(i\) where \(i &gt; k\).</li>
  <li>The first passenger sits in seat \(i\) where \(2 \leq i \leq k -1\).</li>
</ol>

<p>In events a) and c) passenger \(k\) sits in their assigned seat with probability 1. Event b) requires some more careful analysis.</p>

<p>Say passenger \(1\) sits in seat \(i\) such that \(2 \leq i \leq k -1\). In this case, passengers \(2,\ldots,i-1\) all get their assigned seats. Passenger \(i\), now without their assigned seat, will sit uniformly at random in one of the seats numbered \(1, i+1, i+2, \ldots, n\). But this is the same problem as before with passenger \(i\) as the new “drunk passenger” whose assigned seat is \(1\). In this smaller problem, there are \(n-i+1\) passengers and passenger \(k\) in the larger problem is now passenger \(k - i + 1\) in the smaller problem. Therefore in the event that passenger \(1\) sits in seat \(i\), the probability that passenger \(k &gt; i\) sits in their assigned seat is \(P(n+1 - i, k + 1 - i)\).</p>

<p>To put this all into a recurrence relation, we have</p>

\[\begin{equation}
\begin{split}
P(n,k) &amp; = \Pr\left[\text{$k$ sits in seat $k$}|\text{1 sits in seat 1} \right]\cdot \Pr[\text{1 sits in seat 1} ]\\ 
&amp;  + \Pr\left[\text{$k$ sits in seat $k$}|\text{1 sits in seat $i &gt; k$} \right]\cdot \Pr[\text{1 sits in seat $i &gt; k$} ]\\ 
&amp; + \sum_{i=1}^{k-1} \Pr\left[\text{$k$ sits in seat $k$}|\text{1 sits in seat $i$} \right]\cdot \Pr[\text{1 sits in seat $i$} ]\\ 
&amp; = 1 \cdot \frac{1}{n} + 1 \cdot \frac{n-k}{n} + \sum_{i=2}^{k-1}P(n+1 - i, k + 1 - i)\cdot \frac{1}{n}\\ 
&amp; = \frac{n+1 -k}{n} + \frac{1}{n} \sum_{i=2}^{k-1}P(n+1 - i, k + 1 - i)\\ 
&amp; = \frac{n+1 -k}{n} + \frac{1}{n} \sum_{i=1}^{k-2}P(n - i, k - i)\\
\end{split}\label{eq:recurrence}
\end{equation}\]

<p>with base cases \(P(n,1) = \frac{1}{n}\) and \(P(n,2) = \frac{n-1}{n}\) for all \(n\). But how might we go about evaluating (\ref{eq:recurrence})?</p>

<p>Well, by solving for a few small values of \(k\) by hand, we find that \(P(n,3) = \frac{n-2}{n-1}\) and \(P(n,4) = \frac{n-3}{n-2}\). This leads us to the following proposition which we will then prove by induction on \(k\).</p>

<div class="proposition">
For \(n \in \mathbb{Z}_+ \) and integer \(k \leq n\) the probability that passenger \(k\) sits in their assigned seat in the drunk passenger problem is 
\begin{equation}
    \boxed{P(n,k) = \begin{cases}
      \frac{n + 1 - k}{n + 2 - k} &amp; k\geq 2\\ 
      \frac{1}{n} &amp; k = 1.
      \end{cases}}
  \end{equation}
</div>

<p><em>Proof.</em></p>

<p>We will prove this by induction on \(k\). First note the base cases for \(P(n,1)\) holds. Then using (\ref{eq:recurrence}) we have \(P(n,2) = \frac{n-1}{n}\) which agrees with our previous results. These two base cases hold for all \(n \geq 2\). We will now proceed with the inductive hypothesis and assume the result holds for all \(k' &lt; k\). It now suffices to show \(P(n,k) = \frac{n + 1 - k}{n + 2 - k}\).</p>

<p>From (\ref{eq:recurrence}) we have 
\(\begin{equation}  
\begin{split}
P(n,k) &amp; = \frac{n + 1 - k}{n} + \frac{1}{n} \sum_{i=1}^{k-2}P(n - i, k - i)\\ 
       &amp; = \frac{n + 1 - k}{n} + \frac{1}{n} \sum_{i=1}^{k-2}\frac{(n-i) + 1 - (k-i)}{(n-i) + 2 - (k-i)}\\ 
       &amp; = \frac{n + 1 - k}{n} + \frac{1}{n} \sum_{i=1}^{k-2}\frac{n + 1 - k}{n + 2 - k}\\ 
       &amp; = \frac{n + 1 - k}{n} + \frac{k-2}{n}\cdot\frac{n + 1 - k}{n + 2 - k}\\
       &amp; = \frac{n + 1 - k}{n}\left(1  + \frac{k-2}{n + 2 - k}\right)\\
       &amp;  = \frac{n + 1 - k}{n}\left(\frac{n}{n + 2 - k}\right)\\
       &amp; = \frac{n + 1 - k}{n + 2 - k}
\end{split}
\end{equation}\)
as desired.
\(\begin{align*} &amp; \tag*{\(\blacksquare\)} \end{align*}\)</p>

<p>Based on this we do recover the classic result that \(P(n,n) = 1/2\). Taking the derivative of \(P(n,k)\) with respect to \(k\) we can see that it is a decreasing function in \(k\) and that the later in the line you are, the less likely it is you will sit in your assigned seat with a worst case probability of \(1/2\).</p>]]></content><author><name>Connor Colombe</name></author><category term="math" /><summary type="html"><![CDATA[I recently stumbled across a problem that asked for a generalization of the “Drunk Passenger” problem. The classic problem is as follows: Problem Statement There are \(n\) seats on an airplane and \(n\) passengers. The passengers are in a line to board the plane and the \(i\)-th passenger in line is assigned to seat number \(i\) on the plane. However, the first passenger in line is drunk and takes a seat on the plane uniformly at random. The rest of the passengers take a on the plane seat one by one. Passenger \(i \geq 2\) will sit in their assigned seat if it is available, otherwise they will sit in an open seat uniformly at random. What is the probability the \(n\)-th passenger in line sits in their assigned seat?]]></summary></entry><entry><title type="html">Throwing Darts</title><link href="https://ccolombe12.github.io/blog/2023/Throwing_Darts/" rel="alternate" type="text/html" title="Throwing Darts" /><published>2023-04-14T00:00:01+00:00</published><updated>2023-04-14T00:00:01+00:00</updated><id>https://ccolombe12.github.io/blog/2023/Throwing_Darts</id><content type="html" xml:base="https://ccolombe12.github.io/blog/2023/Throwing_Darts/"><![CDATA[<h2 id="problem-statement">Problem Statement</h2>

<p>Jason throws two darts at a dartboard, aiming for the center. The second dart lands farther from the center than the first. If Jason throws a third dart aiming for the center, what is the probability that the third throw is farther from the center than the first? Assume Jason’s skillfulness is constant.</p>

<p><em>Challenge: What if, after the first, his
next \(n − 2\) throws are further from the center than the first? What is the probability that
the \(n\)-th throw is farther from the center than the first?</em></p>
<hr />

<h2 id="solution">Solution</h2>

<p>At first glance, it may seem that the information about the second throw is irrelevant. However, we shall see that that is not the case. We will present two solutions to this problem, both interesting in their own right. The first is perhaps a more traditional computational approach to the problem, and the second uses a cheeky symmetry argument.</p>
<hr />

<h1 id="solution-1">Solution 1</h1>
<p>We will solve the general problem and apply it to the \(n = 3\) case. Let \(R_1,R_2,\ldots, R_n \in [0,1]\) be i.i.d. random variables denoting the radii at which each of the consecutive dart throws land. We would like to evaluate
\(\begin{equation}
  \mathbb{P}\left[R_n &gt; R_1 | R_2 &gt; R_1 \cap \cdots \cap R_{n-1} &gt; R_1\right]
\end{equation}\)</p>

<p>Using Bayes Theorem, we can write this as 
\(\begin{equation}
  \mathbb{P}\left[R_n &gt; R_1 | R_2 &gt; R_1 \cap \cdots \cap R_{n-1} &gt; R_1\right] = \frac{\mathbb{P}\left[R_2 &gt; R_1 \cap \cdots \cap R_{n-1} &gt; R_1\ \cap \left(R_{n} &gt; R_1\right)\right]}{\mathbb{P}\left[R_2 &gt; R_1 \cap \cdots \cap R_{n-1} &gt; R_1\right]} \label{eq:solu}.
\end{equation}\)</p>

<p>Now it suffices to evaluate \(\begin{equation}  
\mathbb{P}\left[ \cap_{i = 2}^n R_{i} &gt; R_1\right] \label{eq:key}.
\end{equation}\)
In order to do this, we will need to know the probability distribution function for \(R\), denoted \(f(r)\). We can deduce this by considering the unit circle (dartboard) and imagine a thin concentric ring at radius \(r\) of width \(dr\). The area of this ring is \(2 \pi r \; dr\), the area of the circle is \(\pi\) and thus the probability of Jason throwing a dart and it landing at radius \(r\) is \(f(r) = 2 r \; dr\) (see Figure 1).</p>

<h3><figure><center>
  <img height="50" src="/assets/img/blog_images/2023-04-14-Throwing_Darts/fig_dart_prob.png" class="img-fluid rounded z-depth-1" zoomable="true" />
</center></figure></h3>
<div class="caption">
    Figure 1. Depicting the small area on a unit dartboard where a dart can land at a radius in $[r,r + dr]$.
</div>
<p>This then implies,</p>

\[\begin{equation}
  \mathbb{P}\left[R &gt; r\right] = \mathbb{P}\left[R \geq r\right] = 1 - r^2.
\end{equation}\]

<p>We can then evaluate (\ref{eq:key}) by conditioning on the value of \(R_1\).</p>

\[\begin{align*}
  \mathbb{P}\left[ \cap_{i = 2}^n R_{i} &gt; R_1\right] &amp; = \int_0^1 (1-r^2)^{n-1} 2 r \; dr \\ 
  &amp; = 2 \int_0^1 (1-r^2)^{n-1} r \; dr  \\
  &amp; = 1/ n.
\end{align*}\]

<p>Based on this, we can write (\ref{eq:solu}) as</p>

\[\begin{align*}
 \mathbb{P}\left[R_n &gt; R_1 | R_2 &gt; R_1 \cap \cdots \cap R_{n-1} &gt; R_1\right] &amp; = \frac{\mathbb{P}\left[R_2 &gt; R_1 \cap \cdots \cap R_{n-1} &gt; R_1\ \cap \left(R_{n} &gt; R_1\right)\right]}{\mathbb{P}\left[R_2 &gt; R_1 \cap \cdots \cap R_{n-1} &gt; R_1\right]}  \\ 
 &amp; = \frac{1/n}{1/(n-1)}\\ 
 &amp; = \frac{n-1}{n}\\ 
 &amp; = \boxed{1 - \frac{1}{n}}.
\end{align*}\]

<p>Plugging in \(n = 3\) yields the solution to our original problem \(\boxed{2/3}\).</p>
<hr />

<h1 id="solution-2">Solution 2</h1>

<p>Imagine that all of the throws have already occurred and we are revealing them one at a time. Up until the final throw, the first throw is the closest to the center of the dartboard. We are asked to find, given this knowledge, what is the probability that the first throw is the closest of all of the throws. At this point, the only way this could <em>not</em> be the case if is the last throw is the closest to the center of the \(n\) throws. Since his skill is constant, the probability that his last throw is the closest to the center is the same as for any other throw, \(1/n\). Therefore, the probability that the first throw was the closest of the \(n\) throws given it was closer than the next \(n-2\) throws is \(\boxed{1 - \frac{1}{n}}\).</p>]]></content><author><name>Connor Colombe</name></author><category term="math" /><summary type="html"><![CDATA[Problem Statement]]></summary></entry><entry><title type="html">An Expected Value Identity</title><link href="https://ccolombe12.github.io/blog/2023/Expected_Value_Identity/" rel="alternate" type="text/html" title="An Expected Value Identity" /><published>2023-04-14T00:00:01+00:00</published><updated>2023-04-14T00:00:01+00:00</updated><id>https://ccolombe12.github.io/blog/2023/Expected_Value_Identity</id><content type="html" xml:base="https://ccolombe12.github.io/blog/2023/Expected_Value_Identity/"><![CDATA[<style type="text/css">
    ol { list-style-type: lower-alpha; }
</style>

<p>I recently decided to brush up on my stochastic processes knowledge and started reading <em>Stochastic Processes</em> by Sheldon Ross, 1995. In the problems section of the first chapter, I came across an expected value identity that I knew was true for non-negative discrete random variables, but I had never used for continuous random variables. Alongside it was a further generalization for higher order moments. Having worked out the problem, I think it was interesting enough to share!</p>
<h2 id="problem-statement">Problem Statement</h2>
<p>Let \(N\) denote a non-negative random variable.</p>
<ol>
  <li>
    <p>) Show that \(\begin{equation}\mathbb{E}[N] = \sum_{k=0}^\infty \mathbb{P}[N &gt; k]\end{equation}\)</p>
  </li>
  <li>
    <p>) In general, show that if \(X\) is a nonnegative random variable with distribution \(F\), then  \(\begin{equation}
  \mathbb{E}[X] = \int_0^{\infty} \overline{F}(x)\; dx
\end{equation}\)</p>
  </li>
</ol>

<p>where \(\overline{F}(x) = 1 - F(x)\).</p>

<ol>
  <li>) and \(\begin{equation}
  \mathbb{E}\left[X^n\right] = \int_0^{\infty}n x^{n-1} \overline{F}(x)\; dx \label{eq: janky expected val}
\end{equation}\)</li>
</ol>

<hr />

<h2 id="solution">Solution</h2>

<p>a. ) Recall the equation for the expected value of a non-negative random variable \(N\), 
\(\begin{equation}
\mathbb{E}[N] = \sum_{k=0}^\infty k \cdot \mathbb{P}[N = k] = \sum_{k=1}^\infty k \cdot \mathbb{P}[N = k] \label{eq:discrete EV}
\end{equation}\)</p>

<p>Note that for a given \(k\), we are adding up \(k\) copies of \(\mathbb{P}[N =k]\). We can visualize this as a grid</p>

\[\begin{align*}
  &amp; \mathbb{P}[N = 1] + \\ 
  &amp; \mathbb{P}[N = 2] + \mathbb{P}[N = 2] + \\ 
  &amp; \mathbb{P}[N = 3] + \mathbb{P}[N = 3] + \mathbb{P}[N = 3] + \\ 
  &amp; \quad \vdots \qquad \qquad \qquad \vdots \qquad \qquad \quad
    \vdots 
  \end{align*}\]

<p>The standard expected value equation (\ref{eq:discrete EV}) assumes that we sum the rows of the above grid. The \(k\)-th row sum being \(k \cdot \mathbb{P}[N = k]\). However, if we consider the column sums, the \(k\)-th column sum is given by</p>

\[\begin{equation*} \mathbb{P}[N = k] + \mathbb{P}[N = k + 1] + \cdots  = \mathbb{P}[N &gt; k - 1]
  \end{equation*}\]

<p>which implies that we can write (\ref{eq:discrete EV}) as</p>

<p>\(\begin{equation*}
    \mathbb{E}[N] = \sum_{k=0}^\infty\mathbb{P}[N &gt; k]
  \end{equation*}\)
  as desired. Note that this proof this proof relied on using the grid of probabilities and summing it in two different ways. This method will not work when \(N\) can take negative values since we can no longer construct the grid.</p>

<p>c.) Note that b.) follows from c.) for \(n = 1\). Let \(n\) be some positive integer and consider the standard equation for \(\mathbb{E}[X^n]\) for a non-negative RV \(X\)</p>

<p>\(\begin{equation}
    \mathbb{E}[X^n] = \int_0^\infty x^n f(x)\; dx \label{eq:cont ev}
  \end{equation}\)<br />
  where \(f(x) = \frac{d F(x)}{dx}\) is the probability density function for the random variable \(X\). We will show using integration by parts that (\ref{eq: janky expected val}) reduces to the standard (\ref{eq:cont ev}).</p>

<p>Consider
  \(\int_0^{\infty}n x^{n-1} \overline{F}(x)\; dx\) and using the standard integration by parts notation let \(u = \overline{F}(x) = 1 - F(x)\) and \(dv = n x^{n-1}\), which implies \(du = - f(x)\; dx\) and \(V = x^n\). Using the standard integration by parts formula \(\int_{a}^b u \;dv  = [uv]^b_a - \int_{a}^b v \; du\), we obtain</p>

\[\begin{align*}
    \int_0^{\infty}n x^{n-1} \overline{F}(x)\; dx &amp; = \lim_{b \to \infty}\left[x^n \left(1 - F(x)\right)\right]_0^b + \int_0^{\infty}x^n f(x)\\ 
    &amp; = (0 - 0) + \int_0^{\infty}x^n f(x)\\ 
    &amp; = \mathbb{E}[X^n]
  \end{align*}\]

<p>as desired!</p>]]></content><author><name>Connor Colombe</name></author><category term="math" /><summary type="html"><![CDATA[I recently decided to brush up on my stochastic processes knowledge and started reading Stochastic Processes by Sheldon Ross, 1995. In the problems section of the first chapter, I came across an expected value identity that I knew was true for non-negative discrete random variables, but I had never used for continuous random variables. Alongside it was a further generalization for higher order moments. Having worked out the problem, I think it was interesting enough to share! Problem Statement Let \(N\) denote a non-negative random variable. ) Show that \(\begin{equation}\mathbb{E}[N] = \sum_{k=0}^\infty \mathbb{P}[N &gt; k]\end{equation}\)]]></summary></entry><entry><title type="html">Optimal Weights</title><link href="https://ccolombe12.github.io/blog/2023/Grain_farmer/" rel="alternate" type="text/html" title="Optimal Weights" /><published>2023-04-11T00:00:01+00:00</published><updated>2023-04-11T00:00:01+00:00</updated><id>https://ccolombe12.github.io/blog/2023/Grain_farmer</id><content type="html" xml:base="https://ccolombe12.github.io/blog/2023/Grain_farmer/"><![CDATA[<h2 id="problem-statement">Problem Statement</h2>
<p>A farmer, who sells grain, has a set of four weights that each weigh an integer number of pounds and a fair balance. She claims that she can weigh any integer number of pounds of grain up to a maximum of \(N\) using just these weights. What is the maximum value of \(N\) and what should the weights be? <em>Challenge: Given \(n\) weights what is the maximum value of \(N(n)\) and what should the weights be?</em></p>
<hr />

<h2 id="solution">Solution</h2>

<p>Let us consider the generalized case and then use it to answer the question for when \(n = 4\). Given \(n\) weights, we would like to pick the value of their individual weights \(w_1,w_2,\ldots, w_n\) such that using the balance we can weigh items of weights \(1,2, \ldots, N(n)\) such that \(N(n)\) is maximized. In this problem we will say that you can <em>weigh</em> an item of weight \(x\) if by placing some our weights on the left side of the balance and potentially some weights on the right side of the balance, the two sides have an absolute weight difference of exactly \(x\). So, how can can we go about determining what the values should be?</p>

<p>First off, we can place an upper bound on what \(N(n)\) by counting the number of distinct weight configurations on the balance. For a given weight, it can be either: on the left side of the balance, on the right side of the balance, or not on the balance. Since this is true for any of our \(n\) weights, there are \(3^n\) configurations of our weights on the balance. How many of these produce a unique weight difference? By symmetry, if we swap the  weights on either side, we produce the same net weight difference so we have over-counted by a factor of 2. The only case where this symmetry argument does not apply is when there are no weights on either side. By this logic, if each weight configuration produces a unique weight difference, then we have shown the upper bound \(\begin{equation}
  N(n) \leq \frac{3^n - 1}{2}.
\end{equation}\)</p>

<p>Great! Now what should our weights be? Suppose we have found that with \(n\) weights we can measure weights up to \(N(n)\). This maximum value must be achieved when all weights are on one side of the balance. Now suppose we can add an additional weight \(w_{n+1}\) to our set. What should it be to maximize our new weighing potential? Well given our current weights, we can’t weigh \(N(n) + 1\) so our new weight ought to allow us to measure this. One such weight that satisfies this can be found</p>

<p>\(\begin{equation}
  w_{n+1} - N(n) = N(n) + 1 \implies w_{n+1} = 2 N(n) + 1.
\end{equation}\)
Using this new weight and the previous weight set, we can achieve all integral weights up to \(N(n) + w_{n+1} = 3 N(n) + 1.\) If we adopt this strategy of choosing the next weight to add to our set, starting at zero weights, we get the recurrence relation \(\begin{align}
  N(n + 1) &amp; = 3 N(n) + 1\\ 
  N(0) &amp; = 0
\end{align}\)</p>

<p>which can we solved to find \(\boxed{N(n) = \frac{3^n - 1}{2}}\), the theoretical upper-bound! It then follows that \(w_{n} =  3^{n-1}\). That is, if we have \(n\) weights the optimal assignment of their weight values are the first \(n\) powers of \(3\) starting at \(0\). Based on this, the solution to our original problem is \(\boxed{N(4) = 40}\) and using weights \(\boxed{\{1,3,9,27\}}\).</p>

<p>Our colleague Brent Austgen cleverly pointed out that your can actually <strong>Double</strong> the value of \(N(n)\) if you double the values of all the weights. Using this strategy, only even numbers are achievable but to measure an odd number you simply determine which two adjacent even numbers the weight falls between! This strategy allows us to reach up to \(3^n - 1\) with \(n\) weights!</p>]]></content><author><name>Connor Colombe</name></author><category term="math" /><summary type="html"><![CDATA[A fun problem about a grain farmer and their balance.]]></summary></entry><entry><title type="html">The Amoeba Problem</title><link href="https://ccolombe12.github.io/blog/2023/Amoeba/" rel="alternate" type="text/html" title="The Amoeba Problem" /><published>2023-04-09T00:00:01+00:00</published><updated>2023-04-09T00:00:01+00:00</updated><id>https://ccolombe12.github.io/blog/2023/Amoeba</id><content type="html" xml:base="https://ccolombe12.github.io/blog/2023/Amoeba/"><![CDATA[<h2 id="problem-statement">Problem Statement</h2>
<p>You start off with one amoeba. Every minute, this amoeba can either:</p>
<ul>
  <li>Do nothing</li>
  <li>Die</li>
  <li>Split into two amoebas</li>
  <li>Split into three amoebas</li>
</ul>

<p>Each of these actions has an equal probability of occurring. All further amoebas behave the
exact same way. What is the probability that the amoebas eventually die off?</p>

<p><em>Challenge: Suppose now the amoeba has the ability to split into up to</em> \(n\) <em>amoebas. For large</em> \(n\)<em>, what is probability that the amoebas eventually die off now?</em></p>
<hr />

<h2 id="solution">Solution</h2>

<p>Let \(p\) be the probability that an amoeba and all of its decedents eventually die out. We can then calculate this probability by conditioning on each of the actions the amoeba could take</p>

\[\begin{align}
p = \frac{1}{4} +\frac{1}{4} p+ \frac{1}{4} p^2 + \frac{1}{4} p^3. \label{eq:n4}
\end{align}\]

<p>Each term on the right hand side representing the events that the amoeba dies, does nothing, splits in two, and splits in three respectively. Since the any two amoeba’s behave independently of one another, if there are \(k\) amoebas, the probability that all \(k\) and their respective descendants dies out is \(p^k\). Solving for \(p\) in (\ref{eq:n4}), yields two solutions in \([0,1]\). Namely, \(p = 1\) and \(p = \sqrt{2} - 1\). But which of the two is the solution?</p>

<p>We can show that that \(p = \sqrt{2} - 1\) is indeed the correct solution by  considering the probability that a given amoeba and its ancestors die out in at most \(k\) minutes, denoted \(P_k\). We can show that for any \(k \geq 1\), \(P_k\) is bounded above by \(\sqrt{2} - 1\) and therefore can never approach \(1\).</p>

<p>We can prove this by induction. Clearly \(P_1 = \frac{1}{4}\) since there is only one way for the a given amoeba to last one minute and \(\frac{1}{4} &lt; \sqrt{2} - 1\). Assume this holds for \(k\). What is the probability a given amoeba and its ancestors die out in at most \(k + 1\) minutes? Well from (\ref{eq:n4}) we have 
\(\begin{align*}
P_{k+1} &amp; = \frac{1}{4} +\frac{1}{4} P_{k}+ \frac{1}{4} P_{k}^2 + \frac{1}{4} P_{k}^3 \\ 
&amp; = \frac{1}{4}\left( 1 + P_k + P_k^2 + P_k^3\right) &amp;&amp; (\text{Plug in } P_k &lt; \sqrt{2} - 1) \\ 
&amp; &lt; \sqrt{2} - 1
\end{align*}\)</p>

<p>completing our inductive step. Thus as \(k \to \infty\) our probability of extinction \(P_\infty = p\) can never get to 1. Implying the only possible solution is \(\boxed{ p = \sqrt{2} - 1}\).</p>

<p>If we consider the case where now the amoeba now has the options to split into as many as \(n\) amoebas, can we get an estimate of \(p\)? It turns out we can!</p>

<p>Consider the generalized form of (\ref{eq:n4})</p>

\[\begin{align}
p &amp; = \frac{1}{n+1}\sum_{k=0}^n p^k\\ 
 &amp; =\frac{1}{n+1}\left(\frac{1-p^{n+1}}{1 - p}\right)\\ 
 \implies &amp; \\ 
 p (n+1) &amp; = \frac{1-p^{n+1}}{1 - p} \label{eq:gen}.
\end{align}\]

<p>Note that for large \(n\), the value of  \(p = \frac{1}{n+1}\) begins to approximate the solution to (\ref{eq:gen}). We plotted the numerical solution to (\ref{eq:gen}) as function of \(n\) (labeled \(F[n]\) in the figure) and the function \(g(n) = 1/n\) and from the figure below we can see they begin to agree very quickly!</p>

<h3><figure><center>
  <img width="300" src="/assets/img/blog_images/2023-04-09-Amoeba/Amoeba_sim.png" class="img-fluid rounded z-depth-1" zoomable="true" />
</center></figure></h3>
<div class="caption">
    Using Mathematica to test our conjecture. The numerical solutions converge very quickly to the predicted value.
</div>]]></content><author><name>Connor Colombe</name></author><category term="math" /><summary type="html"><![CDATA[First post! Solution to a classic interview problem]]></summary></entry></feed>